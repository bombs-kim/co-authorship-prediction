"""
Note:
  'embedding.pth' and 'log.txt' are saved under ./backup/.../ directory.
  The argument parser of this script is automatically generated by
  docopt package using this help message itself.

Usage:
  train_embedding.py symmetric [options]
  train_embedding.py skipgram  [options]
  train_embedding.py (-h | --help)

Options:
  -d --dim <int>            Embedding dimension [default: 128]
  -b --batch <int>          Batch size          [default: 100]
  --lr <float>              Learning rate       [default: 1e-2]
  -e --epochs <int>         Epochs              [default: 1000]
  --max-context <int>       Maximum context length                 [default: 3]
  --neg-sample-num <int>    Number of negative samples per context [default: 10]
  --backup-interval <int>   Interval of model backup [default: 100]
  -s --seed <int>           Random seed [default: 0]
  --device <int>            Cuda device [default: 0]
  --num-workers <int>       Number of processors [default: 4]
  --file <str>              Path to input file [default: data/paper_author.txt]
  --dirname <str>           Directory name to save trained files [default: None]
  -h --help                 Show this screen.
"""

import os
import time

# parsing library for cmd options and arguments https://github.com/docopt/docopt
from docopt import docopt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

from data import FixedLengthContextDataset, HyperedgeDataset
from model import SkipGram, SymmetricEmbedding
from utils import CosineLoss, now_kst, get_dirname


def train(model, loader, dname, epoch_num=100, lr=0.1, backup_interval=100, device=None):
    print("start training")
    optimizer = optim.SparseAdam(model.parameters(), lr=lr)
    recent_loss = 0

    for epoch in range(epoch_num):
        start = time.time()

        for batch_idx, (pos_u, pos_v, neg_v) in enumerate(loader):
            if torch.cuda.is_available():
                pos_u = pos_u.to(device=device)
                pos_v = pos_v.to(device=device)
                neg_v = neg_v.to(device=device)

            optimizer.zero_grad()
            loss = model(pos_u, pos_v, neg_v)

            loss.backward()
            optimizer.step()

            recent_loss = 0.95 * recent_loss + 0.05 * loss.item()

            if (batch_idx+1) % len(loader) == 0:
                join = os.path.join
                # torch.save(model.state_dict(), join(dname, 'embedding.pth'))
                if (epoch+1) % backup_interval == 0:
                    torch.save(model.state_dict(), join(dname, f"embedding_{epoch+1:05d}.pth"))

                log_msg = (f'epoch {epoch+1:2d}: '
                           f'loss {loss.item():6.4f}/{recent_loss:6.4f} {now_kst()}')
                print(log_msg, '\r', end='')
                with open(os.path.join(dname, "log.txt"), 'a') as f:
                    f.write(log_msg + '\n')
                start = time.time()
        print()


def main():
    args = docopt(__doc__)
    np.random.seed(int(args['--seed']))
    torch.manual_seed(int(args['--seed']))
    torch.cuda.manual_seed_all(int(args['--seed']))
    embedding_dim = int(args['--dim'])
    batch_size    = int(args['--batch'])
    lr     = float(args['--lr'])
    epochs = int(args['--epochs'])
    backup_interval = int(args['--backup-interval'])
    device = torch.device(int(args['--device']))
    num_workers = int(args['--num-workers'])
    fpath  = args['--file']
    dname = args['--dirname']
    max_context = int(args['--max-context'])
    neg_sample_num = int(args['--neg-sample-num'])
    
    dset = FixedLengthContextDataset(fpath, max_context, neg_sample_num)
    vocabulary_size = dset.num_authors
    
    # Symmetric vectors are used to compute cosine similarity
    if args['symmetric']:
        model = SymmetricEmbedding(vocabulary_size, embedding_dim)
        if dname == 'None':
            dname = get_dirname('embedding_symmetric')
        else:
            os.makedirs(dname)
    # Word2Vec Skip-gram. Unsymmetric vectors are used to compute cosine similarity
    elif args['skipgram']:
        model = SkipGram(vocabulary_size, embedding_dim)
        if dname == 'None':
            dname = get_dirname('embedding_skipgram')
        else:
            os.makedirs(dname)

    if torch.cuda.is_available():
        model = model.to(device)
    loader = DataLoader(dset, batch_size, num_workers=num_workers)

    train(model, loader, dname, epochs, lr, backup_interval, device)


if __name__ == '__main__':
    main()
