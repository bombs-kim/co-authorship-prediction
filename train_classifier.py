"""
Note:
  'classifier.pth' and 'log.txt' are saved under ./backup/.../ directory.
  The argument parser of this script is automatically generated by
  docopt package using this help message itself.

Usage:
  train_classifier.py (--embedding <str>) (--lstm | --deepset) [options]
  train_classifier.py (-h | --help)

Options:
  --embedding <str>     Path for embedding.pth (required)
  --train-embedding     When set, re-train embedding
  --handle-foreign      When set, make all foreign authors to have the same idx. If there are more than one foreign authors, only one idx remains.
  --enable-all-pools    (DeepSet only option) enable all poolings

  --lstm                When set, use bidirectional LSTM aggregator
  --deepset             When set, use DeepSet aggregator
  --hidden <int>        Hidden size         [default: 128]
  --dropout <float>     Dropout rate        [default: 0.2]

  -b --batch <int>      Batch size          [default: 100]
  --emb-lr <float>      Learning rate for embedding network [default: 1e-4]
  --lr <float>          Learning rate       [default: 1e-3]
  -e --epochs <int>     Epochs              [default: 100]
  -s --seed <int>       Random seed         [default: 0]
  --ratio <float>       Train validation split ratio    [default: 0.8]
  --dirname <str>       Directory name to save trained files [default: None]

  --device <int>        Cuda device         [default: 0]
  -h --help             Show this screen
"""

import os

# parsing library for cmd options and arguments https://github.com/docopt/docopt
from docopt import docopt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

from data import QueryDataset
from model import Classifier
from utils import get_dirname, now_kst, load_embedding


def train_classifier(train_loader, valid_loader, classifier,
                     optimizers, device, epoch, batch_size, logdir=None):
    avg_loss = 0
    loss = 0

    train_correct = 0
    for i, (collabs, labels) in enumerate(train_loader):
        score = classifier(collabs.to(device))
        # L2 loss
        step_loss = (labels.to(device) - score).pow(2)
        loss += step_loss
        avg_loss += step_loss.item()

        with torch.no_grad():
            train_correct += (score.cpu().round() == labels).item()

        if (i+1) % batch_size == 0 or (i+1) == len(train_loader):
            # TODO: reduce memory footprint for BP
            loss /= batch_size
            [optim.zero_grad() for optim in optimizers]
            loss.backward()
            [optim.step() for optim in optimizers]
            loss = 0

        if (i+1) % len(train_loader) == 0:
            correct = 0
            classifier.eval()
            for collabs, labels in valid_loader:
                score = classifier(collabs.to(device)).round()
                correct += (score.cpu() == labels).item()

            acc = (correct / len(valid_loader)) * 100
            train_acc = (train_correct / len(train_loader)) * 100
            classifier.train()

    avg_loss /= len(train_loader)

    log_msg = f'Epoch {epoch+1:d} | Avg Loss: {avg_loss:.6f} | Train Acc: '\
              f'{train_acc:.2f}% | Val Acc: {acc:.2f}% | {now_kst()}'

    if logdir:
        path = os.path.join(logdir, 'log.txt')
        with open(path, 'a') as f:
            f.write(log_msg + '\n')

    return avg_loss, train_acc, acc


def main():
    args = docopt(__doc__)
    train_embedding = args['--train-embedding']
    handle_foreign = args['--handle-foreign']
    enable_all_pools = args['--enable-all-pools']

    np.random.seed(int(args['--seed']))
    torch.manual_seed(int(args['--seed']))
    torch.cuda.manual_seed_all(int(args['--seed']))

    hidden = int(args['--hidden'])
    dropout = float(args['--dropout'])
    batch_size    = int(args['--batch'])
    lr     = float(args['--lr'])
    emb_lr     = float(args['--emb-lr'])
    epochs = int(args['--epochs'])
    device = torch.device(int(args['--device']))
    ratio  = float(args['--ratio'])
    dname = args['--dirname']

    train_dset = QueryDataset(split='train', ratio=ratio,
                              equally_handle_foreign_authors=handle_foreign)
    valid_dset = QueryDataset(split='valid', ratio=ratio,
                              equally_handle_foreign_authors=handle_foreign)
    train_loader = DataLoader(train_dset, batch_size=1, num_workers=1, shuffle=True)
    valid_loader = DataLoader(valid_dset, batch_size=1, num_workers=1, shuffle=False)

    embedding_mode, embedding = load_embedding(
        args['--embedding'], train_embedding, device)
    classifier = Classifier(embedding, hidden, dropout, args['--deepset'],
                            equally_handle_foreign_authors=handle_foreign,
                            enable_all_pools=enable_all_pools)

    if torch.cuda.is_available():
        classifier.to(device)

    emb_params = set(embedding.parameters())
    cls_params = set(classifier.parameters()).difference(emb_params)

    optimizer1 = optim.SparseAdam(emb_params, lr=emb_lr)
    optimizer2 = optim.Adam(cls_params, lr=lr)

    train_embedding = 'on' if train_embedding else 'off'
    if dname == 'None':
        mode = f'{classifier.savename}_emb-{embedding_mode}'\
               f'_trainemb-{train_embedding}'
        dname = get_dirname(mode)
    else:
        os.makedirs(dname, exist_ok=True)
    path = os.path.join(dname, 'log.txt')
    with open(path, 'a') as f:
        f.write(repr(args) + '\n')
    backup_path = os.path.join(dname, 'classifier.pth')

    # TODO: Add checkpoint training feature

    pbar = tqdm(total=epochs, initial=0,
                bar_format="{desc:<5}{percentage:3.0f}%|{bar:10}{r_bar}")
    best_acc = 0
    for epoch in range(epochs):
        avg_loss, train_acc, val_acc = train_classifier(
            train_loader, valid_loader, classifier,
            [optimizer1, optimizer2], device, epoch, batch_size, dname)
        if val_acc > best_acc:
            torch.save(classifier.state_dict(), backup_path)
        pbar.set_description(
            f'Train Loss: {avg_loss:.6f}, Train Acc:{train_acc:.2f} Valid Acc: {val_acc:.2f}%')
        pbar.update(1)

if __name__ == '__main__':
    main()
