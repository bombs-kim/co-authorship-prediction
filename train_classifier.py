"""
Note:
  'classifier.pth' and 'log.txt' are saved under ./backup/.../ directory.
  The argument parser of this script is automatically generated by
  docopt package using this help message itself.

Usage:
  train_classifier.py (--embedding <str>) (--lstm | --deepset) [options]
  train_classifier.py (-h | --help)

Options:
  --embedding <str>     Path for embedding.pth (required)
  --train_embedding     When set, re-train embedding

  --lstm                When set, use bidirectional LSTM aggregator
  --deepset             When set, use DeepSet aggregator
  --hidden <int>        Hidden size         [default: 128]
  --dropout <float>     Dropout rate        [default: 0.2]

  -b --batch <int>      Batch size          [default: 100]
  --lr <float>          Learning rate       [default: 1e-3]
  -e --epochs <int>     Epochs              [default: 100]
  -s --seed <int>       Random seed         [default: 0]
  --ratio <float>       Train validation split ratio    [default: 0.2]

  --device <int>        Cuda device         [default: 0]
  -h --help             Show this screen
"""

import os

# parsing library for cmd options and arguments https://github.com/docopt/docopt
from docopt import docopt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

from data import QueryDataset
from model import SkipGram, Classifier
from utils import get_dirname, now_kst, load_embedding


def train_classifier(train_loader, valid_loader, classifier,
                     optimizers, device, epoch, batch_size, logdir=None):
    pbar = tqdm(total=len(train_loader), initial=0,
                bar_format="{desc:<5}{percentage:3.0f}%|{bar:10}{r_bar}")
    running_loss = 0
    avg_loss = 0
    count = 0
    loss = 0

    for i, (nodes, label) in enumerate(train_loader):
        score = classifier(nodes.to(device))
        step_loss = torch.abs(label.to(device) - score)
        loss += step_loss
        avg_loss += step_loss.item()
        running_loss += step_loss.item()

        if (i+1) % batch_size == 0 or i == len(train_loader) - 1:
            # TODO: reduce memory footprint for BP
            loss /= batch_size
            [optim.zero_grad() for optim in optimizers]
            loss.backward()
            [optim.step() for optim in optimizers]
            loss = 0

        count += 1
        if (i+1) % 1000 == 0:
            correct = 0
            classifier.eval()
            for nodes, label in valid_loader:
                score = classifier(nodes.to(device)).round()
                correct += (score.cpu() == label).item()

            acc = (correct / len(valid_loader)) * 100
            classifier.train()
            pbar.set_description('train_loss: {:.6f}, valid_acc: {:.2f}%'.format(running_loss/count, acc))
            running_loss = 0
            count = 0

        pbar.update(1)

    avg_loss /= len(train_loader)
    last_acc = acc

    log_msg = f'Epoch {epoch:d} | Avg Loss: {avg_loss:.6f} | Val Acc: {last_acc:.2f}% | {now_kst()}'
    print(f'\n', log_msg)
    if logdir:
        path = os.path.join(logdir, 'log.txt')
        with open(path, 'a') as f:
            f.write(log_msg + '\n')

    return avg_loss, last_acc


def main():
    args = docopt(__doc__)
    train_embedding = args['--train_embedding']
    np.random.seed(int(args['--seed']))
    hidden = int(args['--hidden'])
    dropout = float(args['--dropout'])
    batch_size    = int(args['--batch'])
    lr     = float(args['--lr'])
    epochs = int(args['--epochs'])
    device = torch.device(int(args['--device']))
    ratio  = float(args['--ratio'])

    train_dset = QueryDataset(split='train')
    valid_dset = QueryDataset(split='valid')
    train_loader = DataLoader(train_dset, batch_size=1, shuffle=True)
    valid_loader = DataLoader(valid_dset, batch_size=1, shuffle=False)

    embedding_mode, embedding = load_embedding(
        args['--embedding'], train_embedding, device)
    classifier = Classifier(embedding, hidden, dropout, args['--deepset'])

    if torch.cuda.is_available():
        classifier.to(device)

    emb_params = set(embedding.parameters())
    cls_params = set(classifier.parameters()).difference(emb_params)

    optimizer1 = optim.SparseAdam(emb_params, lr=lr)
    optimizer2 = optim.Adam(cls_params, lr=lr)

    train_embedding = 'on' if train_embedding else 'off'
    mode = f'{classifier.savename}_emb-{embedding_mode}'\
           f'_trainemb-{train_embedding}'
    dname = get_dirname(mode)
    path = os.path.join(dname, 'log.txt')
    with open(path, 'a') as f:
        f.write(repr(args) + '\n')
    backup_path = os.path.join(dname, 'classifier.pth')

    # TODO: Add checkpoint training feature
    best_acc = 0
    for epoch in range(epochs):
        avg_loss, val_acc = train_classifier(
            train_loader, valid_loader, classifier,
            [optimizer1, optimizer2], device, epoch, batch_size, dname)
        if val_acc > best_acc:
            torch.save(classifier.state_dict(), backup_path)

if __name__ == '__main__':
    main()
